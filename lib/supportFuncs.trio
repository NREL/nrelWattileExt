name:wattileExportTrainingData
doc:
  For the specified 'predictors', 'targets', and time 'span', export historical data to directory 'dir' for training Wattile models. 'predictors' and 'targets' are any inputs supported by `toRecList`; 'span' is any input supported by `toSpan`. The exported data are consistent with the training data input format required by [Wattile]`https://github.com/NREL/Wattile/blob/main/docs/Data_configs.md`.
  
  Details
  -------
  
  Writes the following files within 'dir':
  
  - '<<Name>> Predictors <<Date Range>>.csv': Time series file(s) containing predictor (input) data in CSV format
  - '<<Name>> Targets <<Date Range>>.csv': Time series file(s) containing target (output) data in CSV format
  - '<<Name>> Config.json': Data set configuration metadata in JSON format
  
  In the filenames, '<<Name>>' is the data set name (which defaults to the name of 'dir'; see *Options*) and '<<Date Range>>' is the date range spanned by each file (with format dependent on the export options selected).
  
  Each CSV file contains a 'Timestamp' column followed by one or more numeric value columns corresponding to either the predictor or the target points. Timestamps are exported in [ISO 8601]`https://en.wikipedia.org/wiki/ISO_8601` standard format, including time zone offset and time zone name. Only numeric data are supported.
  
  The JSON configuration file consists of a single top-level dictionary containing five nested dictionaries:
  
  - 'dates': The data set 'start' and 'end' dates (timestamps)
  - 'predictors': A list of predictor variables, including the following metadata for each:
    - 'id': Unique identifier
    - 'dis': Description; populated via `dis`
    - 'column': Column name in CSV file(s)
    - 'unit': Unit
  - 'targets': A list of target variables, including at minimum
    the `column` field and following the same conventions as `predictors`
  - 'files': Provides a list of all CSV files in the data set, each as an dictionary
    with the following metadata:
    - 'filename': Name of the associated CSV file
    - 'contentType': Either '"predictors"' or '"targets"'
    - 'start': Start of the time range spanned by the file
    - 'end': End of the time range spanned by the file
  - 'export_options': The 'opts' arguments used for the export (provided for repeatability)
  
  The JSON configuration file uses Haystack [Json]`docHaystack::Json` encoding (Version 3 by default; see *Options*). Unicode characters are not escaped.
  
  Options
  -------
  
  Export behavior can be modified by control options passed via 'opts':
  
  - 'appendUnits': Boolean; if 'true' then units will be appended to column names (Default = false)
  - 'clean': Boolean; see `wattileReadHis` (Default = false)
  - 'defVal': Number or NA; see `wattileReadHis` (Default = none)
  - 'interpolate': Boolean; see `wattileReadHis` (Default = false)
  - 'interval': Number; see `wattileReadHis` (Default = none)
  - 'jsonVersion': Str; either 'v3' or 'v4'; see [Json]`docHaystack::Json` (Default = 'v3')
  - 'name': Str; data set name to use in output files (Default = name of 'dir')
  - 'preview': Boolean; if 'true' then return the data set configuration JSON without writing any files (Default = false)
  - 'rollup': Boolean; see `wattileReadHis` (Default = false)
  - 'splitBy': Str; One of "day", "week", "month", or "year" indicating the interval at which to split the data files (Default = no splitting)
  
  The 'appendUnits', 'clean', 'interpolate', 'preview', and 'rollup' options may be also be passed as markers, with 'x' equivalent to 'x:true' and '-x' equivalent to 'x:false'.
  
  Pre-Processing
  --------------
  
  Options 'clean', 'defVal', 'interpolate', 'interval', and 'rollup' control data pre-processing prior to export. See `wattileReadHis` for each option's effect.
  
  Note: interpolation is performed for each exported CSV file independently, therefore, the set of timestamps in target and predictor CSV file for the same date range are not guaranteed to be the same.
func
overridable
src:
  (predictors, targets, span, dir, opts:{}) => do
    // Argument parsing
    predictors = predictors.toRecList.sort((a, b) => a.dis <=> b.dis)
    targets = targets.toRecList.sort((a, b) => a.dis <=> b.dis)
    span = span.toSpan
    if (dir.isStr) dir = parseUri(dir)
  
    // Default options
    opts = {
      appendUnits: false,
      clean:       false,
      interpolate: false,
      jsonVersion: "v3",
      preview:     false,
      rollup:      false
    }.merge(opts)
  
    // T/F options
    opts = opts.set("appendUnits", opts.has("appendUnits") and opts["appendUnits"] != false)
    opts = opts.set("clean",       opts.has("clean")       and opts["clean"]       != false)
    opts = opts.set("interpolate", opts.has("interpolate") and opts["interpolate"] != false)
    opts = opts.set("preview",     opts.has("preview")     and opts["preview"]     != false)
    opts = opts.set("rollup",      opts.has("rollup")      and opts["rollup"]      != false)
  
    // Verify directory
    if (not dir.uriIsDir) throw "'dir' `" + dir.toStr + "` is not a directory."
  
    // Default name = name of directory
    if (opts.missing("name")) opts = opts.set("name", dir.uriName)
  
    // Verify JSON version option
    if (not opts["jsonVersion"].in(["v3", "v4"])) do
      throw "'jsonVersion' option must be either \"v3\" or \"v4\"."
    end
  
    // Time splitting
    if (opts["splitBy"] == null) do
      spans: [span]
      spanLabels: null
    else if (opts->splitBy == "day") do
      spans: span.toDayList(true)
      spanLabels: spans.map(s => s.start.format("YYYY-MM-DD"))
    else if (opts->splitBy == "week") do
      spans: span.toWeekList(true)
      spanLabels: spans.map(s => s.start.format("'Week of 'YYYY-MM-DD"))
    else if (opts->splitBy == "month") do
      spans: span.toMonthList(true)
      spanLabels: spans.map(s => s.start.format("YYYY-MM"))
    else if (opts->splitBy == "year") do
      spans: span.toYearList(true)
      spanLabels: spans.map(s => s.start.format("YYYY"))
    else do
      throw "'splitBy' option must be one of: \"day\", \"week\", \"month\", \"year\"."
    end
  
    // Standardize column labels for output
    setColumnLabel: point => do
      // Column description
      dis: point.dis
  
      // Append units?
      if (opts->appendUnits and point.has("unit")) do
        dis = dis + " (" + point->unit + ")"
      end
  
      // Store 'dis' = column label on export
      point.merge({dis:dis, column:dis, -disMacro})
    end
    targets = targets.map(setColumnLabel)
    predictors = predictors.map(setColumnLabel)
  
    // Internal function to remove units
    hisRemoveUnits: (hisGrid) => do
      hisGrid.hisMap() (v, t) => if (v.isNumber) v.as("") else v
    end
  
    // Options to pass through to wattileReadHis()
    readOpts: opts.findAll((val, key) => key.in(["clean", "defVal", "interpolate", "interval", "rollup"]))
  
    // Internal function to read, process, and export history for specific time range
    hisExportInternal: (points, spanLocal, file) => do
      // Read and pre-process data
      data: wattileReadHis(points, spanLocal, readOpts)
  
      // Remove units
      data = data.hisRemoveUnits
  
      // Timestamp column label
      data = data.addColMeta("ts", {dis:"Timestamp"})
  
      // Write to file
      if (not opts->preview) data.ioWriteCsv(file)
  
      // Return file metadata
      return {
        filename: file.uriName,
        start:    spanLocal.start,
        end:      spanLocal.end
      }
    end
  
    // Export targets
    targetFiles: spans.map() (s, i) => do
      // Output filename
      suffix: if (spanLabels != null) (" " + spanLabels[i]) else ""
      f: opts->name + " " + "Targets" + suffix + ".csv"
  
      // Execute export and return file info
      hisExportInternal(targets, s, dir + f).merge({contentType:"targets"})
    end
  
    // Export predictors
    predictorFiles: spans.map() (s, i) => do
      // Output filename
      suffix: if (spanLabels != null) (" " + spanLabels[i]) else ""
      f: opts->name + " " + "Predictors" + suffix + ".csv"
  
      // Execute export and return file info
      hisExportInternal(predictors, s, dir + f).merge({contentType:"predictors"})
    end
  
    // Formatting function for point records
    formatRecForJSON: rec => removeNull({
        id:     rec->id,
        dis:    rec->dis,
        column: rec->dis,
        unit:   rec["unit"],
        minVal: rec["minVal"],
        maxVal: rec["maxVal"],
        defVal: rec["defVal"]
      })
  
    // Organize predictor and target lists
    pointColOrder: ["id", "dis", "column", "unit", "minVal", "maxVal", "defVal"]
    predictors = predictors.map(formatRecForJSON).toGrid.reorderCols(pointColOrder)
    targets = targets.map(formatRecForJSON).toGrid.reorderCols(pointColOrder)
  
    // Organize file list
    fileColOrder: ["filename", "contentType", "start", "end"]
    files: targetFiles.addAll(predictorFiles).toGrid.reorderCols(fileColOrder)
  
    // Create structure for JSON conversion
    configuration: {
      dates:          {start:span.start, end:span.end},
      predictors:     predictors.toRecList,
      targets:        targets.toRecList,
      files:          files.toRecList,
      export_options: opts.remove("preview")
    }
  
    // Write JSON
    if (not opts->preview) do
      jsonOpts: {noEscapeUnicode}.set(opts->jsonVersion, marker())
      configuration.ioWriteJson(dir + (opts->name + " " + "Config.json"), jsonOpts)
    end
  
    // Return
    configuration
  end
---
name:wattileImportModels
doc:
  Imports records for Wattile model(s) located in 'dirs' and optionally commits them to the database. 'dirs' may be a single uri, a list of uris, a list of anything that can be parsed to a uri, or a grid with a 'uri' column.
  
  Each imported model is a dict with tags:
  
  - 'wattileModel'
  - 'predictors': Grid of predictor metadata (see [documentation]`ext-nrelWattile::doc#modelRecords`)
  - 'targetRef': Optional ref to the prediction target (if available from 'predictors_config.json')
  - 'uri': Path to the model directory
  
  **TO DO:** Defx 'uri'; Defs for the others
  
  Does not populate optional tags 'dis', 'tz', or 'opts' (as this information is not currently included with Wattile models). Returns a list of model records.
  
  Options
  -------
  
  Import behavior can be modified by control options passed via 'opts':
  
  - 'checked': Boolean; throw an error for invalid models (default = 'true')
  - 'commit': Boolean; if 'true' commits the model records to the database (default = 'false')
  - 'conflict': Action on conflicts; one of "skip", "overwrite", "duplicate", or "error" (default = "error")
  
  The 'checked', and 'commit' options may be also be passed as markers, with 'x' equivalent to 'x:true' and '-x' equivalent to 'x:false'.
  
  Conflicts
  ---------
  
  Conflicts with existing 'wattileModel' records (matched by 'uri') are handled based on the 'conflict' option:
  
    - skip: Skip import
    - overwrite: Merge imported metadata onto existing record, overwriting it
    - duplicate: Create a new 'wattileModel' record with duplicate 'uri'
    - error: Throw error
  
  Model Validation
  ----------------
  
  This function performs some basic validation on imported models, including checking directory integrity and verifying the existence of the model's predictor and target points within the SkySpark cluster. Behavior on encountering an invalid model depends on the 'checked' option:
  
  - If 'checked' is 'true', throws an error when an invalid model is encountered
  - If 'checked' is 'false', silently skips invalid models
  
func
overridable
src:
  (dirs, opts:{}) => do
    // Default options
    opts = {checked:true, conflict:"error"}.merge(opts)
  
    // T/F options
    opts = opts.set("checked", opts.has("checked") and opts["checked"] != false)
    opts = opts.set("commit",  opts.has("commit")  and opts["commit"]  != false)
  
    // Action on conflict
    opts = opts.set("conflict", opts->conflict.lower)
    if (not opts->conflict.in(["skip", "overwrite", "duplicate", "error"])) do
      throw "'conflict' option must be one of: \"skip\", \"overwrite\", \"duplicate\""
    end
  
    // Coerce dirs to Uri list
    if (dirs.isGrid) do
      dirs = dirs.colToList("uri")
    else do
      dirs = dirs.toList
    end
  
    // Map dirs to records
    models: dirs.map() dir => do
      // Check directory uri
      if (not dir.isUri) dir = dir.parseUri(opts->checked)
  
      // Validate directory integrity
      try do
        if (dir.isNull) throw "Model directory is null"
        if (ioInfo(dir).missing("dir")) throw "`" + dir.toStr + "` is not a directory"
        if (ioDir(dir).isEmpty) throw "Model directory `" + dir.toStr + "` is empty or does not exist"
        if (ioDir(dir).find(f => f->name=="predictors_config.json", false) == null) throw "Model directory `" + dir.toStr + "` is missing \"predictors_config.json\""
      catch (ex) do
        if (opts->checked) throw (ex)
        return null // Invalid model
      end
  
      // Model dict
      model: {wattileModel, uri:dir}
  
      // Build up model dict with predictors and targets
      try do
        // Read config
        predictorConfig: ioReadJson(dir + "predictors_config.json", {v3}) // TO DO: Update to v4 encoding
  
        // Temporary: if config is a list, assume it is a list of predictors
        if (predictorConfig.isList) do
          predictorConfig = {predictors:predictorConfig}
        end
  
        // Get predictors
        predictors: predictorConfig->predictors
          .map() p => do
            // Check for required tags
            if (p.missing("id")) throw "Predictor " + p.dis + " is missing 'id' key."
            if (p.missing("column")) throw "Predictor " + p.dis + " is missing 'column' key."
  
            // Default 'dis' to "description" key value, if present
            if (p.missing("dis") and p.has("description")) do
              p = p.set("dis", p->description)
            end
  
            // Resolve ID and return
            p.set("id", wattileResolveRef(p["id"]))
          end
  
        // Predictors grid
        predictors = predictors.toGrid.keepCols(["id", "dis", "column", "unit", "minVal", "maxVal", "defVal"])
  
        // Store to model
        model = model.set("predictors", predictors)
  
        // Get target
        if (predictorConfig.has("target")) do
          // Target ref
          if (predictorConfig->target.missing("id")) throw "Target " + predictorConfig->target.dis + " is missing 'id' key."
          targetRef: wattileResolveRef(predictorConfig->target->id)
  
          // Store to model
          model = model.set("targetRef", targetRef)
  
          // Target's original units
          if (predictorConfig->target.has("unit")) do
            // Store to model
            model = model.set("unit", predictorConfig->target->unit)
          end
        end
  
        // TO DO: Also import model name as 'dis'? From where?
        // TO DO: Range cleaning options?
  
      catch (ex) do
        if (opts->checked) throw (ex)
        return null // Invalid model
      end
  
      // Return
      model
    end
  
    // Resolve existing models
    models = models
      .findAll(m => m != null)
      .map() m => do
        // Existing?
        existingModel: read(wattileModel and uri==m->uri, false)
  
        // No existing model
        if (existingModel == null) return m
  
        // Handle existing model
        if (opts->conflict == "overwrite") return existingModel.merge(m)
        if (opts->conflict == "duplicate") return m
        if (opts->conflict == "skip") return null
  
        // "error" or any other case
        throw "'wattileModel' for uri `" + m->uri + "` already exists."
      end
  
    // Commit?
    if (opts->commit) do
      diffs: models.map() m => if (m.has("id")) do
        diff(m, m.remove("id"))
      else do
        diff(null, m, {add})
      end
      models = diffs.commit
    end
  
    // Return models
    return models
  end
---
name:wattileReadHis
doc:
  For the specified 'points' and time 'span', read and pre-process historical data for use with a Wattile model. 'points' is any input supported by `toRecList`; 'span' is any input supported by `toSpan`. Returns a history grid.
  
  Options
  -------
  
  Data pre-processing is controlled by the following options, passed via 'opts':
  
  - 'clean': Boolean; if 'true' then data will be range-cleaned prior to export by removing values outside the range defined by each point's `minVal` and `maxVal` tags (Default = false)
  - 'defVal': Number or NA; governs range cleaning behavior; see below (Default = none)
  - 'interpolate': Boolean; if 'true' then data will be interpolated prior to export (Default = false)
  - 'interval': Number; optional interval at which to interpolate and/or roll up the data (Default = none)
  - 'rollup': Boolean; if 'true' then a history rollup will be applied prior to export (Default = false)
  
  The 'clean', 'interpolate', and 'rollup' options may be also be passed as markers, with 'x' equivalent to 'x:true' and '-x' equivalent to 'x:false'. If 'rollup' is 'true', then 'interval' must also be specified.
  
  Range Cleaning
  --------------
  
  If 'clean' is 'true', values outside the range '[minVal, maxVal]', as defined by each point's `minVal` and `maxVal` tags, are removed from the data set. (Missing 'minVal' and 'maxVal' tags imply permissible minimum and maximum values of negative and positive infinity, respectively.)
  
  If the 'defVal' option is provided, values removed are replaced with 'defVal'; otherwise, they are replaced with Null. Optionally, 'defVal' may be passed as a tag on each point; 'defVal' point tags override the global option on a per-point basis.
  
  Range cleaning is performed prior to interpolation and/or rollup.
  
  Interpolation
  -------------
  
  If 'interpolate' is 'true', the entire history grid is interpolated via `hisInterpolate`, following SkySpark's normal interpolation rules. If the 'interval' option is also specified, then additional interpolation is performed *selectively* to ensure each interval of data contains at least one value for each input point. Interpolation is performed prior to rollup.
  
  
  Rollup
  ------
  
  If the 'rollup' option is provided, data are rolled up at the specified 'interval' using `hisRollupAuto`.
  
func
overridable
src:
  (points, span, opts:{}) => do
    // Argument parsing
    points = points.toRecList
    span = span.toSpan
  
    // Default options
    opts = {clean:false, interpolate:false}.merge(opts)
  
    // T/F options
    opts = opts.set("clean",       opts.has("clean")       and opts["clean"]       != false)
    opts = opts.set("interpolate", opts.has("interpolate") and opts["interpolate"] != false)
    opts = opts.set("rollup",      opts.has("rollup")      and opts["rollup"]      != false)
  
    // Internal function: Perform range cleaning (only if 'clean' option is set)
    if (opts->clean) do
      hisRangeCleanInternal: (hisGrid, point) => do
        // Get default value
        if (point.has("defVal")) do
          defVal: point->defVal
        else if (opts.has("defVal")) do
          defVal: opts->defVal
        else do
          defVal: null
        end
  
        // Get range
        minVal: if (point.has("minVal")) point->minVal else negInf()
        maxVal: if (point.has("maxVal")) point->minVal else posInf()
  
        // No default value: remove
        if (defVal == null) do
          hisGrid = hisGrid.hisFindAll((v, t) => v != na() and v >= minVal.to(v) and v <= maxVal.to(v))
  
        // Default value: replace
        else do
          hisGrid = hisGrid.hisMap() (v, t) => do
            if (v == na() or (v >= minVal.to(v) and v <= maxVal.to(v))) do
              return v
            else do
              return defVal
            end
          end
        end
  
        // Return
        return hisGrid
      end
    else do
      hisRangeCleanInternal: (hisGrid, point) => hisGrid
    end
  
    // Internal function: Interpolate intervals with no data (only if 'interpolate' option is set)
    if (opts->interpolate and opts.has("interval")) do
      hisInterpolateAtIntervalInternal: (hisGrid) => do
        // Find gaps (intervals with no data)
        gaps: hisGrid
          .hisRollup(count, opts->interval)
          .hisFindAll((val, ts) => val == 0)
          .keepCols(["ts"])
        
        // Interpolate gaps
        hisJoin([hisGrid, gaps]).hisInterpolate
      end
    else do
      hisInterpolateAtIntervalInternal: (hisGrid) => hisGrid
    end
    
    // Internal function: Final history interpolation (only if 'interpolate' option is set)
    if (opts->interpolate) do
      hisInterpolateInternal: (hisGrid) => hisGrid.hisInterpolate
    else do
      hisInterpolateInternal: (hisGrid) => hisGrid
    end
  
    // Internal function: Roll up history (only if 'interval' option is set)
    if (opts.has("interval")) do
      hisRollupInternal: (hisGrid) => do
        // Data type
        kind: hisGrid.col("v0").meta["kind"]
        
        // Numeric
        if (kind == "Number") do
          return hisGrid.hisRollupAuto(opts->interval)
  
        // Unsupported
        else do
          throw "Unsupported kind \"" + kind + "\" for point: " + hisGrid.col("v0").meta.dis
        end
      end
    else do
      hisRollupInternal: (hisGrid) => hisGrid
    end
  
    // Internal function: Convert units
    hisConvertUnits: (hisGrid, point) => do
      hisGrid.hisMap() (v, t) => if (v.isNumber) v.to(point["unit"]) else v
    end
  
    // Read each history as individual hisGrid
    data: xq()
      .xqDefine("readspan", span)
      .xqReadByIdsList(points.toRecIdList)
      .xqMap("p => {id:p->id, hisGrid:p.toRecId.hisRead(readspan, {-limit})}")
      .xqExecute
  
    // Process histories
    data = data.map() dict => do
      // Get corresponding point
      p: points.find(x => x->id == dict->id)
      
      // Extract history grid
      history: dict->hisGrid
  
      // Check for empty history
      if (history.hisClip.isEmpty) throw "No history found for point \"" + p.dis + "\" and span " + format(span)
  
      // Preserve original 'dis', if applicable
      // Needed to enforce column names on CSV export
      if (p.has("dis")) do
        history = history.addColMeta("v0", {dis:p->dis, -disMacro})
      end
  
      // Range clean, rollup, enforce units, set column label
      history = history
        .hisRangeCleanInternal(p)
        .hisInterpolateAtIntervalInternal
        .hisRollupInternal
        .hisConvertUnits(p)
    end
  
    // Join, interpolate, clip to window, and return
    hisJoin(data)
      .hisInterpolateInternal
      .hisClip
  end
---
name:wattileResolveRef
doc:
  Resolve 'refStr' into a valid `haystack::Ref`. If the Ref does not exist in the cluster, throw an error.
  
  Handles any of the following:
  
  - With or without leading '@'
  - With or without trailing description
  - Absolute or relative
  - (Relative refs) with or without '"r:"' prefix
func
overridable
src:
  (refStr) => do
    // Convert to string
    refStr = refStr.toStr
  
    // Trim 'dis' (if any)
    refStr = refStr.split(" ")[0]
  
    // Special cases: relative Refs
    if (refStr.startsWith("r:")) refStr = refStr[2..-1]
    if (refStr.startsWith("@r:")) refStr = refStr[3..-1]
  
    // Parse
    ref: parseRef(refStr)
  
    // Resolve
    try do
      // Try local database
      ref = ref.toRec.toRecId
    catch (ex) do
      // Try cluster via XQuery
      refList: xq().xqReadByIdsList(ref.toList).xqExecute
  
      // Record not found
      if (refList.isEmpty) do
        throw "Ref " + ref.toStr + " cannot be found anywhere in the cluster."
      else if (refList.size > 1) do
        throw "Ref " + ref.toStr + " did not resolve to a unique record!" // This shouldn't ever happen
      else do
        ref = refList.first.toRecId
      end
    end
  end
---
name:wattileSyncHis
doc:
  Syncs Wattile prediction history for 'points' and 'span', using the specified Wattile Python 'task'.
  
  - 'points' may be anything accepted by `toRecIdList`.
  - 'span' may be anything acceptable by `toSpan`, or Null to sync all history after each point's 'hisEnd'.
  
  Each point must define valid tags `wattileModelRef` and `wattileQuantile`. If run within a task, will report task progress.
  
  **TO DO:** Defs for wattileModelRef, wattileQuantile
  
  Sync
  ----
  
  The default sync behavior is:
  
  1. Execute a prediction call to each Wattile model for the specified 'span'.
  2. Extract the 'horizon = 0' prediction data for each point based on its 'wattileQuantile'.
  3. Drop any predictions with timestamps prior to each point's 'hisEnd'.
  4. Write new prediction history is written persistently to each point.
  
  This behavior can be modified via 'opts'; see below.
  
  Options
  -------
  
  Sync behavior can be modified by control options passed via 'opts':
  
  - 'forecast': Boolean; also write forecast data (default = 'false')
  - 'forecastOnly': Boolean; *only* write forecast data (default = 'false')
  - 'limit': Number; limits the length of time span to sync when 'span' is Null (default = None)
  - 'overwrite': Boolean; allows existing history to be overwritten (ignores 'hisEnd') (default = 'false')
  
  All options may be also be passed as markers, with 'x' equivalent to 'x:true' and '-x' equivalent to 'x:false'. If 'forecastOnly = true', the 'forecast' option is ignored. To avoid warning spam in the logs, the 'overwrite' option also sets the `hisWrite` 'noWarn' flag.
  
  Forecasts
  ---------
  
  If either the 'forecast' or the 'forecastOnly' option is 'true', then forecasts (data for 'horizon > 0' in Wattile results) are also written to each point. Only the forecast from the most recent Wattile prediction in the results (most recent value of the 'timestamp' column) is written. Forecasts are always written transiently by setting the `hisWrite` 'forecast' flag.
  
  Calculated Span
  ---------------
  
  If 'span' is Null, then the time span to sync is calculated for each set of points grouped by 'wattileModelRef'. For each group:
  
  - Start of span: Equals the earliest 'hisEnd' among points in the group
  - End of span: Equals the output of `now` or the start of span plus 'limit' (when specified), whichever is earlier
  
  When this function is called from a task or job to keep predictions up-to-date, best practice is to specify 'limit' to avoid extremely large sync batches.
func
overridable
src:
  (points, task, span:null, opts:{}) => do
    // Default options
    opts = {}.merge(opts)
  
    // T/F options
    opts = opts.set("overwrite",     opts.has("overwrite")     and opts["overwrite"]     != false)
    opts = opts.set("forecast",      opts.has("forecast")      and opts["forecast"]      != false)
    opts = opts.set("forecastOnly",  opts.has("forecastOnly")  and opts["forecastOnly"]  != false)
  
    // If span is unspecified, never overwrite
    if (span == null) opts = opts.set("overwrite", false)
  
    // What to write
    writeHistory: not opts->forecastOnly
    writeForecast: opts->forecast or opts->forecastOnly
  
    // Points
    points = points.toRecIdList.readByIds
  
    // Validate points
    points.each() p => do
      // Required tags
      if (p.missing("wattileModelRef")) throw "Point \"" + p.dis + "\" is missing 'wattileModelRef' tag"
      if (p.missing("wattileQuantile")) throw "Point \"" + p.dis + "\" is missing 'wattileQuantile' tag"
      if (p.missing("his")) throw "Point \"" + p.dis + "\" is missing 'his' tag"
      if (p.missing("tz")) throw "Point \"" + p.dis + "\" is missing 'tz' tag"
    end
  
    // Models
    models: points
      .map(p => p->wattileModelRef.toRec)
      .unique("id")
  
    // Span
    if (span != null) span = span.toSpan
  
    // Iterate models
    modifiedPoints: models.toRecList.flatMap() model => do
      // Task progress
      taskProgress({model:model.dis})
      
      // Get associated points
      pointsLocal: points.findAll(p => p->wattileModelRef == model->id)
  
      // Prediction span
      if (span == null) do
        // Only keep points with 'hisEnd'
        pointsLocal = pointsLocal.findAll(p => p.has("hisEnd"))
  
        // Ensure not empty
        if (pointsLocal.isEmpty) do
          logWarn(
            {name:"nrelWattile", funcTrace:"wattileSyncHis"},
            "No points with 'hisEnd' tag found for model " + model.dis + "; skipping."
          )
          return []
        end
  
        // Oldest 'hisEnd' to present, limited by 'limit' if applicable
        spanLocalStart: pointsLocal.toGrid.colToList("hisEnd").sort.first
        spanLocalEnd: now().toTimeZone(spanLocalStart.tz)
        
        if (opts.has("limit") and spanLocalEnd > (spanLocalStart + opts->limit)) do
          spanLocalEnd = (spanLocalStart + opts->limit)
        else if (spanLocalStart > spanLocalEnd) do
          return [] // Skip if up-to-date
        end
        
        spanLocal: spanLocalStart..spanLocalEnd
        
      else do
        // User-specified span
        spanLocal: span
      end
  
      // Call predict
      data: task.taskSend({action: "predict", model:model, span:spanLocal}).futureGet()
  
      // Write prediction history
      if (writeHistory) do
        pointsLocal.each() p => do
          // Filter predictions
          history: data
            .findAll(row => row->horizon == 0 and row->quantile == p->wattileQuantile)
            .keepCols(["pred_ts", "pred_val"])
            .reorderCols(["pred_ts", "pred_val"])
            .renameCol("pred_ts", "ts")
  
          // Convert timezone
          history = history.map(row => row.set("ts", row->ts.toTimeZone(p->tz)))
  
          // Convert units
          history = history.hisMap(v => if (v.isNA) v else v.to(p["unit"]))
  
          // Write data
          if (opts->overwrite) do
            // All history
            hisWrite(history, p, {noWarn})
          else if (p.missing("hisEnd")) do
            // All history
            hisWrite(history, p)
          else do
            // New history only
            history
              .hisFindAll((val, ts) => ts > p->hisEnd)
              .hisWrite(p)
          end
        end
      end
  
      // Write prediction forecast
      if (writeForecast) do
        pointsLocal.each() p => do
          // Most recent prediction timestamp
          mostRecentTime: data.colToList("timestamp").sortr.first
  
          // Filter predictions
          history: data
            .findAll(row => row->timestamp == mostRecentTime and row->horizon > 0 and row->quantile == p->wattileQuantile)
            .keepCols(["pred_ts", "pred_val"])
            .reorderCols(["pred_ts", "pred_val"])
            .renameCol("pred_ts", "ts")
  
          // Convert timezone
          history = history.map(row => row.set("ts", row->ts.toTimeZone(p->tz)))
  
          // Convert units
          history = history.hisMap(v => if (v.isNA) v else v.to(p["unit"]))
  
          // Write data
          hisWrite(history, p, {forecast})
        end
      end
  
      // Return modified points
      pointsLocal.toRecList
    end
  
    // History sync then return modified point records
    hisSync()
    modifiedPoints.flatten.toRecIdList.readByIds
  end