name:wattileExportTrainingData
doc:
  For the specified 'predictors', 'targets', and time 'span', export historical data to directory 'dir' for training Wattile models. 'predictors' and 'targets' are any inputs supported by `toRecList`; 'span' is any input supported by `toSpan`. The exported data are consistent with the training data input format required by [Wattile]`https://github.com/NREL/Wattile/blob/main/docs/Data_configs.md`.
  
  Details
  -------
  
  Writes the following files within 'dir':
  
  - '<<Name>> Predictors <<Date Range>>.csv': Time series file(s) containing predictor (input) data in CSV format
  - '<<Name>> Targets <<Date Range>>.csv': Time series file(s) containing target (output) data in CSV format
  - '<<Name>> Config.json': Data set configuration metadata in JSON format
  
  In the filenames, '<<Name>>' is the data set name (which defaults to the name of 'dir'; see *Options*) and '<<Date Range>>' is the date range spanned by each file (with format dependent on the export options selected).
  
  Each CSV file contains a 'Timestamp' column followed by one or more numeric value columns corresponding to either the predictor or the target points. Timestamps are exported in [ISO 8601]`https://en.wikipedia.org/wiki/ISO_8601` standard format, including time zone offset and time zone name. Only numeric data are supported.
  
  The JSON configuration file consists of a single top-level dictionary containing five nested dictionaries:
  
  - 'dates': The data set 'start' and 'end' dates (timestamps)
  - 'predictors': A list of predictor variables, including the following metadata for each:
    - 'id': Unique identifier
    - 'dis': Description; populated via `dis`
    - 'column': Column name in CSV file(s)
    - 'unit': Unit
  - 'targets': A list of target variables, including at minimum
    the `column` field and following the same conventions as `predictors`
  - 'files': Provides a list of all CSV files in the data set, each as an dictionary
    with the following metadata:
    - 'filename': Name of the associated CSV file
    - 'contentType': Either '"predictors"' or '"targets"'
    - 'start': Start of the time range spanned by the file
    - 'end': End of the time range spanned by the file
  - 'export_options': The 'opts' arguments used for the export (provided for repeatability)
  
  The JSON configuration file uses Haystack [Json]`docHaystack::Json` encoding (Version 3 by default; see *Options*). Unicode characters are not escaped.
  
  Options
  -------
  
  Export behavior can be modified by control options passed via 'opts':
  
  - 'appendUnits': Boolean; if 'true' then units will be appended to column names (Default = false)
  - 'clean': Boolean; see `wattileReadHis` (Default = false)
  - 'batch'; Integer; see `wattileReadHis` (Default = none)
  - 'defVal': Number or NA; see `wattileReadHis` (Default = none)
  - 'interpolate': Boolean; see `wattileReadHis` (Default = false)
  - 'interval': Number; see `wattileReadHis` (Default = none)
  - 'jsonVersion': Str; either 'v3' or 'v4'; see [Json]`docHaystack::Json` (Default = 'v3')
  - 'name': Str; data set name to use in output files (Default = name of 'dir')
  - 'preview': Boolean; if 'true' then return the data set configuration JSON without writing any files (Default = false)
  - 'removeNA': Boolean; see `wattileReadHis` (Default = false)
  - 'rollup': Boolean; see `wattileReadHis` (Default = false)
  - 'splitBy': Str; One of "day", "week", "month", or "year" indicating the interval at which to split the data files (Default = no splitting)
  - 'timeout': Number; see `wattileReadHis` (Default = none)
  - 'warn': Boolean; see `wattileReadHis` (Default = true)
  
  Boolean options may be also be passed as markers, with 'x' equivalent to 'x:true' and '-x' equivalent to 'x:false'.
  
  Pre-Processing
  --------------
  
  Options 'clean', 'defVal', 'interpolate', 'interval', and 'rollup' control data pre-processing prior to export. See `wattileReadHis` for each option's effect.
  
  Note: interpolation is performed for each exported CSV file independently, therefore, the set of timestamps in target and predictor CSV file for the same date range are not guaranteed to be the same.
  
  Tips
  ----
  
  - If you are trying to read history from remote projects via Arcbeam and keep seeing an error that XQuery did not return the correct number of points, experiment with increasing 'timeout' and/or decreasing 'batch'. See `wattileReadHis` for details.
func
overridable
src:
  (predictors, targets, span, dir, opts:{}) => do
    // Argument parsing
    predictors = predictors.toRecList.sort((a, b) => a.dis <=> b.dis)
    targets = targets.toRecList.sort((a, b) => a.dis <=> b.dis)
    span = span.toSpan
    if (dir.isStr) dir = parseUri(dir)
  
    // Default options
    opts = {
      appendUnits: false,
      clean:       false,
      interpolate: false,
      jsonVersion: "v3",
      preview:     false,
      removeNA:    false,
      rollup:      false,
      warn:        true
    }.merge(opts)
  
    // T/F options
    opts = opts.set("appendUnits", opts.has("appendUnits") and opts["appendUnits"] != false)
    opts = opts.set("clean",       opts.has("clean")       and opts["clean"]       != false)
    opts = opts.set("interpolate", opts.has("interpolate") and opts["interpolate"] != false)
    opts = opts.set("preview",     opts.has("preview")     and opts["preview"]     != false)
    opts = opts.set("removeNA",    opts.has("removeNA")    and opts["removeNA"]    != false)
    opts = opts.set("rollup",      opts.has("rollup")      and opts["rollup"]      != false)
    opts = opts.set("warn",        opts.has("warn")        and opts["warn"]        != false)
  
    // Verify directory
    if (not dir.uriIsDir) throw "'dir' `" + dir.toStr + "` is not a directory."
  
    // Default name = name of directory
    if (opts.missing("name")) opts = opts.set("name", dir.uriName)
  
    // Verify JSON version option
    if (not opts["jsonVersion"].in(["v3", "v4"])) do
      throw "'jsonVersion' option must be either \"v3\" or \"v4\"."
    end
  
    // Time splitting
    if (opts["splitBy"] == null) do
      spans: [span]
      spanLabels: null
    else if (opts->splitBy == "day") do
      spans: span.toDayList(true)
      spanLabels: spans.map(s => s.start.format("YYYY-MM-DD"))
    else if (opts->splitBy == "week") do
      spans: span.toWeekList(true)
      spanLabels: spans.map(s => s.start.format("'Week of 'YYYY-MM-DD"))
    else if (opts->splitBy == "month") do
      spans: span.toMonthList(true)
      spanLabels: spans.map(s => s.start.format("YYYY-MM"))
    else if (opts->splitBy == "year") do
      spans: span.toYearList(true)
      spanLabels: spans.map(s => s.start.format("YYYY"))
    else do
      throw "'splitBy' option must be one of: \"day\", \"week\", \"month\", \"year\"."
    end
  
    // Standardize column labels for output
    setColumnLabel: point => do
      // Column description
      dis: point.dis
  
      // Append units?
      if (opts->appendUnits and point.has("unit")) do
        dis = dis + " (" + point->unit + ")"
      end
  
      // Store 'dis' = column label on export
      point.merge({dis:dis, column:dis, -disMacro})
    end
    targets = targets.map(setColumnLabel)
    predictors = predictors.map(setColumnLabel)
  
    // Internal function to remove units
    hisRemoveUnits: (hisGrid) => do
      hisGrid.hisMap() (v, t) => if (v.isNumber) v.as("") else v
    end
  
    // Options to pass through to wattileReadHis()
    readOptNames: ["clean", "batch", "defVal", "interpolate", "interval", "removeNA", "rollup", "timeout", "warn"]
    readOpts: opts.findAll((val, key) => key.in(readOptNames))
  
    // Internal function to read, process, and export history for specific time range
    hisExportInternal: (points, spanLocal, file) => do
      // Read and pre-process data
      data: wattileReadHis(points, spanLocal, readOpts)
  
      // Remove units
      data = data.hisRemoveUnits
  
      // Timestamp column label
      data = data.addColMeta("ts", {dis:"Timestamp"})
  
      // Write to file
      if (not opts->preview) data.ioWriteCsv(file)
  
      // Return file metadata
      return {
        filename: file.uriName,
        start:    spanLocal.start,
        end:      spanLocal.end
      }
    end
  
    // Export targets
    targetFiles: spans.map() (s, i) => do
      // Output filename
      suffix: if (spanLabels != null) (" " + spanLabels[i]) else ""
      f: opts->name + " " + "Targets" + suffix + ".csv"
  
      // Execute export and return file info
      hisExportInternal(targets, s, dir + f).merge({contentType:"targets"})
    end
  
    // Export predictors
    predictorFiles: spans.map() (s, i) => do
      // Output filename
      suffix: if (spanLabels != null) (" " + spanLabels[i]) else ""
      f: opts->name + " " + "Predictors" + suffix + ".csv"
  
      // Execute export and return file info
      hisExportInternal(predictors, s, dir + f).merge({contentType:"predictors"})
    end
  
    // Formatting function for point records
    formatRecForJSON: rec => removeNull({
        id:     rec->id,
        dis:    rec->dis,
        column: rec->dis,
        unit:   rec["unit"],
        minVal: rec["minVal"],
        maxVal: rec["maxVal"],
        defVal: rec["defVal"]
      })
  
    // Organize predictor and target lists
    pointColOrder: ["id", "dis", "column", "unit", "minVal", "maxVal", "defVal"]
    predictors = predictors.map(formatRecForJSON).toGrid.reorderCols(pointColOrder)
    targets = targets.map(formatRecForJSON).toGrid.reorderCols(pointColOrder)
  
    // Organize file list
    fileColOrder: ["filename", "contentType", "start", "end"]
    files: targetFiles.addAll(predictorFiles).toGrid.reorderCols(fileColOrder)
  
    // Create structure for JSON conversion
    configuration: {
      dates:          {start:span.start, end:span.end},
      predictors:     predictors.toRecList,
      targets:        targets.toRecList,
      files:          files.toRecList,
      export_options: opts.remove("preview")
    }
  
    // Write JSON
    if (not opts->preview) do
      jsonOpts: {noEscapeUnicode}.set(opts->jsonVersion, marker())
      configuration.ioWriteJson(dir + (opts->name + " " + "Config.json"), jsonOpts)
    end
  
    // Return
    configuration
  end
---
name:wattileImportModels
doc:
  Imports records for Wattile model(s) located in 'dirs' and optionally commits them to the database. 'dirs' may be a single uri, a list of uris, a list of anything that can be parsed to a uri, or a grid with a 'uri' column.
  
  Each imported model is a dict with tags:
  
  - 'wattileModel'
  - 'wattilePredictors': Grid of predictor metadata (see [documentation]`ext-nrelWattile::doc#modelRecords`)
  - 'wattileTargetRef': Optional ref to the prediction target (if available from 'predictors_target_config.json')
  - 'uri': Path to the model directory
  - 'unit': Optional unit for the prediction output
  
  Does not populate optional tags 'dis', 'tz', or 'wattileOpts' (as this information is not currently included with Wattile models). Returns a list of model records.
  
  Options
  -------
  
  Import behavior can be modified by control options passed via 'opts':
  
  - 'checked': Boolean; throw an error for invalid models (default = 'true')
  - 'commit': Boolean; if 'true' commits the model records to the database (default = 'false')
  - 'conflict': Action on conflicts; one of "skip", "overwrite", "duplicate", or "error" (default = "error")
  
  The 'checked', and 'commit' options may be also be passed as markers, with 'x' equivalent to 'x:true' and '-x' equivalent to 'x:false'.
  
  Conflicts
  ---------
  
  Conflicts with existing 'wattileModel' records (matched by 'uri') are handled based on the 'conflict' option:
  
    - skip: Skip import
    - overwrite: Merge imported metadata onto existing record, overwriting it
    - duplicate: Create a new 'wattileModel' record with duplicate 'uri'
    - error: Throw error
  
  Model Validation
  ----------------
  
  This function performs some basic validation on imported models, including checking directory integrity and verifying the existence of the model's predictor and target points within the SkySpark cluster. Behavior on encountering an invalid model depends on the 'checked' option:
  
  - If 'checked' is 'true', throws an error when an invalid model is encountered
  - If 'checked' is 'false', silently skips invalid models
  
func
overridable
src:
  (dirs, opts:{}) => do
    // Default options
    opts = {checked:true, conflict:"error"}.merge(opts)
  
    // T/F options
    opts = opts.set("checked", opts.has("checked") and opts["checked"] != false)
    opts = opts.set("commit",  opts.has("commit")  and opts["commit"]  != false)
  
    // Action on conflict
    opts = opts.set("conflict", opts->conflict.lower)
    if (not opts->conflict.in(["skip", "overwrite", "duplicate", "error"])) do
      throw "'conflict' option must be one of: \"skip\", \"overwrite\", \"duplicate\""
    end
  
    // Coerce dirs to Uri list
    if (dirs.isGrid) do
      dirs = dirs.colToList("uri")
    else do
      dirs = dirs.toList
    end
  
    // Map dirs to records
    models: dirs.map() dir => do
      // Check directory uri
      if (not dir.isUri) dir = dir.parseUri(opts->checked)
  
      // Verify directory integrity
      try do
        if (dir.isNull) throw "Model directory is null"
        if (ioInfo(dir).missing("dir")) throw "`" + dir.toStr + "` is not a directory"
        if (ioDir(dir).isEmpty) throw "Model directory `" + dir.toStr + "` is empty or does not exist"
        if (ioDir(dir).find(f => f->name=="predictors_target_config.json", false) == null) throw "Model directory `" + dir.toStr + "` is missing \"predictors_target_config.json\""
      catch (ex) do
        if (opts->checked) throw (ex)
        return null // Invalid model
      end
  
      // Model dict
      model: {wattileModel, uri:dir, dis:uriBasename(dir)}
  
      // Build up model dict with predictors and targets
      try do
        // Read config
        predictorConfig: ioReadJson(dir + "predictors_target_config.json", {v3}) // TO DO: Update to v4 encoding
  
        // Get predictors
        predictors: predictorConfig->predictors
          .map() p => do
            // Check for required tags
            if (p.missing("id")) throw "Predictor " + p.dis + " is missing 'id' key."
            if (p.missing("column")) throw "Predictor " + p.recDisWithId + " is missing 'column' key."
  
            // Default 'dis' to "description" key value, if present
            if (p.missing("dis") and p.has("description")) do
              p = p.set("dis", p->description)
            end
  
            // Resolve ID and return
            p.set("id", wattileResolveRef(p["id"]))
          end
  
        // Predictors grid
        predictors = predictors.toGrid.keepCols(["id", "dis", "column", "unit", "minVal", "maxVal", "defVal"])
  
        // Store to model
        model = model.set("wattilePredictors", predictors)
  
        // Get target
        if (predictorConfig.has("target")) do
          // Target ref
          if (predictorConfig->target.missing("id")) throw "Target " + predictorConfig->target.dis + " is missing 'id' key."
          target: wattileResolveRef(predictorConfig->target->id)
  
          // Store to model
          model = model.set("wattileTargetRef", target)
  
          // Target's original units
          if (predictorConfig->target.has("unit")) do
            // Store to model
            model = model.set("unit", predictorConfig->target->unit)
          end
        end
  
      catch (ex) do
        if (opts->checked) throw (ex)
        return null // Invalid model
      end
  
      // Return
      model
    end
  
    // Resolve existing models
    models = models
      .findAll(m => m != null)
      .map() m => do
        // Existing?
        existingModel: read(wattileModel and uri==m->uri, false)
  
        // No existing model
        if (existingModel == null) return m
  
        // Handle existing model
        if (opts->conflict == "overwrite") return existingModel.merge(m)
        if (opts->conflict == "duplicate") return m
        if (opts->conflict == "skip") return null
  
        // "error" or any other case
        throw "'wattileModel' for uri `" + m->uri + "` already exists."
      end
  
    // Commit?
    if (opts->commit) do
      diffs: models.map() m => if (m.has("id")) do
        diff(m, m.remove("id"))
      else do
        diff(null, m, {add})
      end
      models = diffs.commit
    end
  
    // Return models
    return models
  end
---
name:wattileReadHis
doc:
  For the specified 'points' and time 'span', read and pre-process historical data for use with a Wattile model. 'points' is any input supported by `toRecList`; 'span' is any input supported by `toSpan`. Returns a history grid.
  
  Options
  -------
  
  Data pre-processing is controlled by the following options, passed via 'opts':
  
  - 'clean': Boolean; if 'true' then data will be range-cleaned prior to export by removing values outside the range defined by each point's `minVal` and `maxVal` tags (Default = false)
  - 'batch': Integer; optional batch size for XQuery; see below (Default = none)
  - 'defVal': Number or NA; governs range cleaning behavior; see below (Default = none)
  - 'interpolate': Boolean; if 'true' then data will be interpolated (Default = false)
  - 'interval': Number; optional interval at which to interpolate and/or roll up the data (Default = none)
  - 'removeNA': Boolean; if 'true' then NA values will be removed (Default = false)
  - 'rollup': Boolean; if 'true' then a history rollup will be applied (Default = false)
  - 'timeout': Number; optional timeout for `xq`; see below (Default = none)
  - 'warn': Boolean; if 'true' log warnings; if 'false' suppress them (Default = true)
  
  Boolean options may be also be passed as markers, with 'x' equivalent to 'x:true' and '-x' equivalent to 'x:false'. If 'rollup' is 'true', then 'interval' must also be specified.
  
  Range Cleaning
  --------------
  
  If 'clean' is 'true', values outside the range '[minVal, maxVal]', as defined by each point's `minVal` and `maxVal` tags, are removed from the data set. (Missing 'minVal' and 'maxVal' tags imply permissible minimum and maximum values of negative and positive infinity, respectively.)
  
  If the 'defVal' option is provided, values removed are replaced with 'defVal'; otherwise, they are replaced with Null. Optionally, 'defVal' may be passed as a tag on each point; 'defVal' point tags override the global option on a per-point basis.
  
  Range cleaning is performed prior to NA removal, interpolation, and/or rollup.
  
  NA Removal
  ----------
  
  If 'removeNA' is 'true', NA values will be removed from the data set, including any NA values generated from 'defVal' during range cleaning.
  
  NA removal is performed prior to interpolation and/or rollup.
  
  Interpolation
  -------------
  
  If 'interpolate' is 'true', the entire history grid is interpolated via `hisInterpolate`, following SkySpark's normal interpolation rules. If the 'interval' option is also specified, then additional interpolation is performed *selectively* to ensure each interval of data contains at least one value for each input point. Interpolation is performed prior to rollup.
  
  Rollup
  ------
  
  If the 'rollup' option is provided, data are rolled up at the specified 'interval' using `hisRollupAuto`.
  
  Handling Large History Reads
  ----------------------------
  
  Large or computationally-intensive history reads from remote projects can cause unexpected XQuery behavior, such as silent timeouts or keep-alive resets of the Arcbeam websocket. When this happens, you may see the an error message like this:
  
    Expected X points; XQuery returned Y.
  
  Two options can help:
  
  - 'timeout' is passed through to `xq`; increase it to allow XQueries more time to complete
  - 'batch' sets the maximum number of points read per XQuery; decrease it to spread out the history reads over more XQueries
  
  Note: smaller 'batch' size reduces the chance of individual XQuery failures but increases both the total number of XQueries and the total function execution time.
func
overridable
src:
  (points, span, opts:{}) => do
    // Argument parsing
    points = points.toRecList
    span = span.toSpan
    
    // Check points
    if (points.isEmpty) do
      throw "'points' cannot be empty."
    end
  
    // Default options
    opts = {
      clean: false,
      interpolate: false,
      removeNA: false,
      rollup: false,
      warn: true
    }.merge(opts)
  
    // T/F options
    opts = opts.set("clean",       opts.has("clean")       and opts["clean"]       != false)
    opts = opts.set("interpolate", opts.has("interpolate") and opts["interpolate"] != false)
    opts = opts.set("removeNA",    opts.has("removeNA")    and opts["removeNA"]    != false)
    opts = opts.set("rollup",      opts.has("rollup")      and opts["rollup"]      != false)
    opts = opts.set("warn",        opts.has("warn")        and opts["warn"]        != false)
    
    // XQuery options
    if (opts.has("timeout")) do
      xqOpts: {timeout:opts->timeout}
    else do
      xqOpts: null
    end
    
    // XQuery read batches
    if (opts.has("batch")) do
      batchSize: floor(opts->batch)
      numBatch: ceil(points.size / batchSize)
      batches: (0..(numBatch-1)).map() b => do
        idxLow: b * batchSize
        idxHigh: idxLow + batchSize - 1
        getSafe(points, idxLow..idxHigh)
      end
    else do
      batches: [points]
    end
    
    // Internal function: Perform range cleaning (only if 'clean' option is set)
    if (opts->clean) do
      hisRangeCleanInternal: (hisGrid, point) => do
        // Get default value
        if (point.has("defVal")) do
          defVal: point->defVal
        else if (opts.has("defVal")) do
          defVal: opts->defVal
        else do
          defVal: null
        end
        
        // Warn if default value is NA and removeNA is set
        if (opts->warn and defVal == na() and opts->removeNA) do
          logWarn(
            {name:"nrelWattile", funcTrace:"wattileReadHis"},
            "Option 'removeNA' = true overrides a default value ('defVal' option) of NA; " +
            "NA data for " + point.recDisWithId + " will be replaced with Null."
          )
        end
  
        // Get range
        minVal: if (point.has("minVal")) point->minVal else negInf()
        maxVal: if (point.has("maxVal")) point->maxVal else posInf()
  
        // No default value: remove
        if (defVal == null) do
          hisGrid = hisGrid.hisFindAll((v, t) => v != na() and v >= minVal.to(v) and v <= maxVal.to(v))
  
        // Default value: replace
        else do
          hisGrid = hisGrid.hisMap() (v, t) => do
            if (v == na() or (v >= minVal.to(v) and v <= maxVal.to(v))) do
              return v
            else do
              return defVal
            end
          end
        end
  
        // Return
        return hisGrid
      end
    else do
      hisRangeCleanInternal: (hisGrid, point) => hisGrid
    end
  
    // Internal function: NA removal
    if (opts->removeNA) do
      hisRemoveNaInternal: (hisGrid) => hisGrid.hisFindAll(v => v != na())
    else do
      hisRemoveNaInternal: (hisGrid) => hisGrid
    end
  
    // Internal function: Interpolate intervals with no data (only if 'interpolate' option is set)
    if (opts->interpolate and opts.has("interval")) do
      hisInterpolateAtIntervalInternal: (hisGrid) => do
        // Find gaps (intervals with no data)
        gaps: hisGrid
          .hisRollup(count, opts->interval)
          .hisFindAll((val, ts) => val == 0)
          .keepCols(["ts"])
  
        // Interpolate gaps
        hisJoin([hisGrid, gaps]).hisInterpolate
      end
    else do
      hisInterpolateAtIntervalInternal: (hisGrid) => hisGrid
    end
  
    // Internal function: Final history interpolation (only if 'interpolate' option is set)
    if (opts->interpolate) do
      hisInterpolateInternal: (hisGrid) => hisGrid.hisInterpolate
    else do
      hisInterpolateInternal: (hisGrid) => hisGrid
    end
  
    // Internal function: Roll up history (only if 'interval' option is set)
    if (opts.has("interval")) do
      hisRollupInternal: (hisGrid) => do
        // Data type
        kind: hisGrid.col("v0").meta["kind"]
  
        // Numeric
        if (kind == "Number") do
          return hisGrid.hisRollupAuto(opts->interval)
  
        // Unsupported
        else do
          throw "Unsupported kind \"" + kind + "\" for point: " + hisGrid.col("v0").meta.recDisWithId
        end
      end
    else do
      hisRollupInternal: (hisGrid) => hisGrid
    end
  
    // Internal function: Convert units
    hisConvertUnits: (hisGrid, point) => do
      hisGrid.hisMap() (v, t) => if (v.isNumber) v.to(point["unit"]) else v
    end
  
    // Run XQuery in batches
    data: batches
      .map(batchPoints => xq(xqOpts)
        .xqDefine("readspan", span)
        .xqReadByIdsList(batchPoints.toRecIdList)
        .xqMap("p => {id:p->id, hisGrid:p.toRecId.hisRead(readspan, {-limit, hisKeepEmptyCols})}")
        .xqExecute  
      ).flatten
      
    // Verify all points found
    if (data.size < points.size) do
      throw "Expected " + points.size.toStr + " points; XQuery returned " + data.size + "."
    end
    
    // Process histories
    data = data.map() dict => do
      // Get corresponding point
      p: points.find(x => x->id == dict->id)
  
      // Extract history grid
      history: dict->hisGrid.addMeta({hisKeepEmptyCols})
  
      // Check for empty history
      if (opts->warn and history.hisClip.isEmpty) do
        logWarn(
          {name:"nrelWattile", funcTrace:"wattileReadHis"},
          "No history found for point " + p.recDisWithId +
          " and span " + format(span)
        )
      end
  
      // Preserve original 'dis', if applicable
      // Needed to enforce column names on CSV export
      if (p.has("dis")) do
        history = history.addColMeta("v0", {dis:p->dis, -disMacro})
      end
  
      // Range clean, rollup, enforce units, set column label
      history = history
        .hisRemoveNaInternal
        .hisRangeCleanInternal(p)
        .hisInterpolateAtIntervalInternal
        .hisRollupInternal
        .hisConvertUnits(p)
    end
  
    // Join, interpolate, clip to window, and return
    hisJoin(data)
      .hisInterpolateInternal
      .hisClip
  end
---
name:wattileResolveRef
doc:
  Resolve 'refStr' into a valid `haystack::Ref`. If the Ref does not exist in the cluster, throw an error.
  
  Handles any of the following:
  
  - With or without leading '@'
  - With or without trailing description
  - Absolute or relative
  - (Relative refs) with or without '"r:"' prefix
func
overridable
src:
  (refStr) => do
    // Convert to string
    refStr = refStr.toStr
  
    // Trim 'dis' (if any)
    refStr = refStr.split(" ")[0]
  
    // Special cases: relative Refs
    if (refStr.startsWith("r:")) refStr = refStr[2..-1]
    if (refStr.startsWith("@r:")) refStr = refStr[3..-1]
  
    // Parse
    ref: parseRef(refStr)
  
    // Resolve
    try do
      // Try local database
      ref = ref.toRec.toRecId
    catch (ex) do
      // Try cluster via XQuery
      refList: xq().xqReadByIdsList(ref.toList).xqExecute
  
      // Record not found
      if (refList.isEmpty) do
        throw "Ref @" + ref.toStr + " cannot be found anywhere in the cluster."
      else if (refList.size > 1) do
        throw "Ref @" + ref.toStr + " did not resolve to a unique record!" // This shouldn't ever happen
      else do
        ref = refList.first.toRecId
      end
    end
  end
---
name:wattileSyncHis
doc:
  Syncs Wattile prediction history for 'points' and 'span', using the specified Wattile Python 'task'.
  
  - 'points' may be anything accepted by `toRecIdList`.
  - 'span' may be anything acceptable by `toSpan`, or Null to sync all history after each point's 'hisEnd'.
  
  Each point must define valid tags `wattileModelRef` and `wattileQuantile`. If run within a task, will report task progress.
  
  Sync
  ----
  
  The default sync behavior is:
  
  1. Execute a prediction call to each Wattile model for the specified 'span'.
  2. Extract the 'horizon = 0' prediction data for each point based on its 'wattileQuantile'.
  3. Drop any predictions with timestamps prior to each point's 'hisEnd'.
  4. Write new prediction history is written persistently to each point.
  
  This behavior can be modified via 'opts'; see below.
  
  Options
  -------
  
  Sync behavior can be modified by control options passed via 'opts':
  
  - 'delay': Number; delay from the present for syncing predictions (default = '0s')
  - 'forecast': Boolean; also write forecast data (default = 'false')
  - 'forecastOnly': Boolean; *only* write forecast data (default = 'false')
  - 'hotPeriod': Number; optional hot period for syncing predictions (default = None)
  - 'limit': Number; limits the length of time span to sync when 'span' is Null (default = None)
  - 'overwrite': Boolean; allows existing history to be overwritten (ignores 'hisEnd') (default = 'false')
  
  Boolean options may be also be passed as markers, with 'x' equivalent to 'x:true' and '-x' equivalent to 'x:false'. If 'forecastOnly = true', the 'forecast' option is ignored. To avoid warning spam in the logs, the 'overwrite' and 'hotPeriod' options also set the `hisWrite` 'noWarn' flag.
  
  Forecasts
  ---------
  
  If either the 'forecast' or the 'forecastOnly' option is 'true', then forecasts (data for 'horizon > 0' in Wattile results) are also written to each point. Only the forecast from the most recent Wattile prediction in the results (most recent value of the 'timestamp' column) is written. Forecasts are always written transiently by setting the `hisWrite` 'forecast' flag.
  
  Calculated Span
  ---------------
  
  If 'span' is Null, then the time span to sync is calculated for each set of points grouped by 'wattileModelRef'. For each group:
  
  - Start of span: Equals the earliest 'hisEnd' among points in the group or the beginning of the hot period, whichever is earlier
  - End of span: Equals the output of `now` minus 'delay' or the start of span plus 'limit' (when specified), whichever is earlier
  
  When this function is called from a task or job to keep predictions up-to-date, best practice is to specify 'limit' to avoid extremely large sync batches.
  
  Sync Delay
  ----------
  
  The 'delay' option delays syncing of predictions in "real time" to ensure complete input data, specified as a duration measuring backwards in time from the present (as returned by `now`). The minimum recommended 'delay' is the amount of time needed for predictor data to stabilize, *e.g.* for predictor point histories to sync and be written to disk. This helps ensure that the predictions are computed from complete and valid data.
  
  If 'span' is specified (non-Null), the 'delay' option is ignored.
  
  Hot Period
  ----------
  
  The 'hotPeriod' option functions similarly to the hot period for rules: during the hot period the synced predictions are continuously refreshed. This is accomplished by clearing point history within the hot period immediately prior to writing the new predictions. Like 'delay', 'hotPeriod' is specified as a duration measuring backwards in time from the present.
  
  Setting 'hotPeriod' greater than 'limit' will prevent predictions from being fully synced through the present. If 'span' is specified (non-Null), the 'hotPeriod' option is ignored.
func
overridable
src:
  (points, task, span:null, opts:{}) => do
    // Default options
    opts = {delay:0s}.merge(opts)
  
    // T/F options
    opts = opts.set("overwrite",     opts.has("overwrite")     and opts["overwrite"]     != false)
    opts = opts.set("forecast",      opts.has("forecast")      and opts["forecast"]      != false)
    opts = opts.set("forecastOnly",  opts.has("forecastOnly")  and opts["forecastOnly"]  != false)
  
    // Task is running?
    inTask: taskIsRunning()
  
    // If span is unspecified, never overwrite
    if (span == null) opts = opts.set("overwrite", false)
  
    // What to write
    writeHistory: not opts->forecastOnly
    writeForecast: opts->forecast or opts->forecastOnly
  
    // Sync delay
    delay: opts->delay.to(1s)
  
    // Hot period
    if (opts.has("hotPeriod")) do
      hotPeriod: opts->hotPeriod.to(1s)
    else do
      hotPeriod: null
    end
  
    // Span limit
    if (opts.has("limit")) do
      spanLimit: opts->limit.to(1s)
    else do
      spanLimit: null
    end
  
    // Warn if hot period > limit
    if (hotPeriod != null and spanLimit != null and hotPeriod > spanLimit) do
      logWarn(
        {name:"nrelWattile", funcTrace:"wattileSyncHis"},
        "Option 'hotPeriod' = " + format(hotPeriod) +
        " is greater than option 'limit' = " + format(spanLimit) +
        "; synced predictions will lag real time."
      )
    end
  
    // Error flag
    ok: true
  
    // Points
    points = points.toRecIdList.readByIds
  
    // Validate points
    points.each() p => do
      // Required tags
      if (p.missing("wattileModelRef")) throw "Point \"" + p.recDisWithId + "\" is missing 'wattileModelRef' tag"
      if (p.missing("wattileQuantile")) throw "Point \"" + p.recDisWithId + "\" is missing 'wattileQuantile' tag"
      if (p.missing("his")) throw "Point \"" + p.recDisWithId + "\" is missing 'his' tag"
      if (p.missing("tz")) throw "Point \"" + p.recDisWithId + "\" is missing 'tz' tag"
    end
  
    // Models
    models: points.map(p => p->wattileModelRef.toRec)
    if (models.isEmpty) do
      throw "No Wattile model records found!"
    else do
      models = models.unique("id")
    end
  
    // Span
    if (span != null) span = span.toSpan
  
    // Iterate models
    modifiedPoints: models.toRecList.flatMap() (model, i) => do
      // Task progress
      if (inTask) do
        progPct: round(10 * (i / models.size * 100%)) / 10 // Round to 0.1%
        taskProgress({percent:progPct, model:model.dis})
      end
  
      // Get associated points
      pointsLocal: points.findAll(p => p->wattileModelRef == model->id)
  
      // Prediction span
      if (span == null) do
        // Only keep points with 'hisEnd'
        pointsLocal = pointsLocal.findAll(p => p.has("hisEnd"))
  
        // Ensure not empty
        if (pointsLocal.isEmpty) do
          logWarn(
            {name:"nrelWattile", funcTrace:"wattileSyncHis"},
            "No points with 'hisEnd' tag found for model " + model.recDisWithId + "; skipping."
          )
          return []
        end
  
        // Start: oldest 'hisEnd' or hot period start, whichever is older
        spanLocalStart: pointsLocal.toGrid.colToList("hisEnd").sort.first
        if (hotPeriod != null and spanLocalStart >= (now() - hotPeriod)) do
          spanLocalStart = (now() - hotPeriod)
        end
  
        // End: the present, minus the sync delay (if any)
        spanLocalEnd: now().toTimeZone(spanLocalStart.tz) - delay
  
        // Span limit
        if (spanLimit != null and spanLocalEnd > (spanLocalStart + spanLimit)) do
          spanLocalEnd = (spanLocalStart + opts->limit)
        else if (spanLocalStart > spanLocalEnd) do
          return [] // Skip if up-to-date
        end
  
        // Return span
        spanLocal: spanLocalStart..spanLocalEnd
  
      else do
        // User-specified span
        spanLocal: span
      end
  
      // Task progress
      if (inTask) do
        taskProgress({percent:progPct, model:model.dis, span:spanLocal.format})
      end
  
      // Call predict
      try do
        data: task.taskSend({action: "predict", model:model, span:spanLocal}).futureGet()
      catch (ex) do
        logErr(
          {name:"nrelWattileExt", funcTrace:"wattileSyncHis"},
          "Prediction failed for model " + model.recDisWithId + ".",
          ex
        )
        ok = false
        return []
      end
  
      // Write prediction history
      if (writeHistory) do
        pointsLocal.each() p => do
          // Filter predictions
          history: data
            .findAll(row => row->horizon == 0 and row->quantile == p->wattileQuantile)
            .keepCols(["pred_ts", "pred_val"])
            .reorderCols(["pred_ts", "pred_val"])
            .renameCol("pred_ts", "ts")
  
          // Convert timezone
          history = history.map(row => row.set("ts", row->ts.toTimeZone(p->tz)))
  
          // Convert units
          history = history.hisMap(v => if (v.isNA) v else v.to(p["unit"]))
  
          // Write data
          if (opts->overwrite) do
            // All history
            hisWrite(history, p, {noWarn})
          else if (p.missing("hisEnd")) do
            // All history
            hisWrite(history, p)
          else if (hotPeriod != null) do
            // New history, using hot period
            hotPeriodStart: now() - hotPeriod
  
            // Remove existing point history in hot period; filter by hot period
            if (p->hisEnd >= hotPeriodStart) do
              p.hisRemove(hotPeriodStart..now())
              history = history.hisFindAll((val, ts) => ts >= hotPeriodStart)
  
            // No existing history in hot period; use hisEnd
            else do
              history = history.hisFindAll((val, ts) => ts > p->hisEnd)
            end
  
            // Write
            history.hisWrite(p, {noWarn})
          else do
            // New history only
            history
              .hisFindAll((val, ts) => ts > p->hisEnd)
              .hisWrite(p)
          end
        end
      end
  
      // Write prediction forecast
      if (writeForecast) do
        pointsLocal.each() p => do
          // Most recent prediction timestamp
          mostRecentTime: data.colToList("timestamp").sortr.first
  
          // Filter predictions
          history: data
            .findAll(row => row->timestamp == mostRecentTime and row->horizon > 0 and row->quantile == p->wattileQuantile)
            .keepCols(["pred_ts", "pred_val"])
            .reorderCols(["pred_ts", "pred_val"])
            .renameCol("pred_ts", "ts")
  
          // Convert timezone
          history = history.map(row => row.set("ts", row->ts.toTimeZone(p->tz)))
  
          // Convert units
          history = history.hisMap(v => if (v.isNA) v else v.to(p["unit"]))
  
          // Write data
          hisWrite(history, p, {forecast})
        end
      end
  
      // Return modified points
      pointsLocal.toRecList
    end
  
    // Check for error
    if (not ok) do
      throw "Error(s) occurred during Wattile prediction; see log."
    else if (inTask) do
      taskProgress({percent:100%, message:"Complete"})
    end
  
    // History sync then return modified point records
    hisSync()
    modifiedPoints.flatten.toRecIdList.readByIds
  end
